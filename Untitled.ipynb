{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import tqdm\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"News.csv\")\n",
    "dj = pd.read_csv('DowJones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[news.Date.isin(dj.Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj.set_index('Date').diff(periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj['Date'] = dj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj.drop(['High', 'Low', 'Close', 'Volume', 'Adj Close'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = dj[dj.Open.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "headlines = []\n",
    "for i, dj_row  in dj.iterrows():\n",
    "    dj_date = dj_row['Date']\n",
    "    price.append(dj_row['Open'])\n",
    "    news_selected_date = news[news.Date==dj_date]\n",
    "    \n",
    "    temp=''\n",
    "    for str_headline in news_selected_date['News'].values:\n",
    "        temp+=str_headline  \n",
    "\n",
    "    headlines.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_df = pd.DataFrame(data=headlines, index=dj.Date,columns=['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'0,0', '00', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\$', ' $ ', text)\n",
    "    text = re.sub(r'u s ', ' united states ', text)\n",
    "    text = re.sub(r'u n ', ' united nations ', text)\n",
    "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
    "    text = re.sub(r'j k ', ' jk ', text)\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    text = re.sub(r' yr ', ' year ', text)\n",
    "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
    "    text = re.sub(r'0km ', '0 km ', text)\n",
    "    text = re.sub(r'b ', ' ', text)\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_df['cleaned_headlines']=headlines_df['headlines'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for headline in headlines_df['cleaned_headlines'].values:\n",
    "    all_words.update(headline.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54895"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NUM_WORDS = 55000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(headlines_df['cleaned_headlines'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988, 1988)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54389 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "#MAX_NUM_WORDS=180000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "print(max(len(s) for s in sequences))\n",
    "print(min(len(s) for s in sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988, 150)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54390, 100)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54390, 100), 150)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape, MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(LSTM(128))\n",
    "#model.add(Dense(600))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001,clipvalue=1.0), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 150, 100)          5439000   \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 148, 64)           19264     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 74, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 70, 64)            20544     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 5,577,753\n",
      "Trainable params: 138,753\n",
      "Non-trainable params: 5,439,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize opening prices (target values)\n",
    "max_price = max(price)\n",
    "min_price = min(price)\n",
    "mean_price = np.mean(price)\n",
    "def normalize(price):\n",
    "    return ((price-min_price)/(max_price-min_price))\n",
    "norm_price = []\n",
    "for p in price:\n",
    "    norm_price.append(normalize(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1689 samples, validate on 299 samples\n",
      "Epoch 1/10\n",
      "1689/1689 [==============================] - 61s 36ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
      "Epoch 2/10\n",
      "1689/1689 [==============================] - 60s 35ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0148 - val_mean_squared_error: 0.0148\n",
      "Epoch 3/10\n",
      "1300/1689 [======================>.......] - ETA: 13s - loss: 0.0054 - mean_squared_error: 0.0054"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, norm_price, batch_size=100, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "news = pd.read_csv(\"News.csv\")\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "news = pd.read_csv(\"News.csv\")\n",
      "news.head()\n",
      "news = pd.read_csv(\"News.csv\")\n",
      "dj = pd.read_csv('DowJones.csv')\n",
      "news = news[news.Date.isin(dj.Date)]\n",
      "news.head()\n",
      "dj = dj.set_index('Date').diff(periods=1)\n",
      "dj\n",
      "dj['Date'] = dj.index\n",
      "dj\n",
      "dj = dj.reset_index(drop=True)\n",
      "dj\n",
      "dj = dj.drop(['High', 'Low', 'Close', 'Volume', 'Adj Close'], 1)\n",
      "dj\n",
      "dj = dj[dj.Open.notnull()]\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in tqdm(dj.iterrows()):\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    headlines.append(list(news_selected_date['News'].values))\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tdqm\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tdqm\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tdqm\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tdqm\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tqdm\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in tqdm(dj.iterrows()):\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    headlines.append(list(news_selected_date['News'].values))\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in tqdm(dj.iterrows()):\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    headlines.append(list(news_selected_date['News'].values))\n",
      "news = pd.read_csv(\"News.csv\")\n",
      "dj = pd.read_csv('DowJones.csv')\n",
      "news = news[news.Date.isin(dj.Date)]\n",
      "dj = dj.set_index('Date').diff(periods=1)\n",
      "dj['Date'] = dj.index\n",
      "dj = dj.reset_index(drop=True)\n",
      "dj = dj.drop(['High', 'Low', 'Close', 'Volume', 'Adj Close'], 1)\n",
      "dj = dj[dj.Open.notnull()]\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in tqdm(dj.iterrows()):\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    headlines.append(list(news_selected_date['News'].values))\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in dj.iterrows():\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    headlines.append(list(news_selected_date['News'].values))\n",
      "headlines\n",
      "news_selected_date['News'].values\n",
      "news_selected_date['News']\n",
      "news_selected_date['News'].shape\n",
      "news_selected_date['News']\n",
      "type(news_selected_date['News'])\n",
      "news_selected_date['News'].values\n",
      "news_selected_date['News'].values.shape\n",
      "news_selected_date['News'].values[0]\n",
      "news_selected_date['News'].values[0:2]\n",
      "news_selected_date['News'].values.map(lamda x: ''.join(x))\n",
      "news_selected_date['News'].values.map(lambda x: ''.join(x))\n",
      "news_selected_date['News'].map(lambda x: ''.join(x))\n",
      "news_selected_date['News'].values\n",
      "for str_headline in news_selected_date['News'].values:\n",
      "    print(str_headline)\n",
      "temp=''\n",
      "for str_headline in news_selected_date['News'].values:\n",
      "    temp+=str_headline\n",
      "print(temp)\n",
      "dj_date\n",
      "news\n",
      "news['Data']==dj_date\n",
      "news['Data']\n",
      "news['Date']==dj_date\n",
      "news[news['Date']==dj_date]\n",
      "price = []\n",
      "headlines = []\n",
      "for i, dj_row  in dj.iterrows():\n",
      "    dj_date = dj_row['Date']\n",
      "    price.append(dj_row['Open'])\n",
      "    news_selected_date = news[news.Date==dj_date]\n",
      "    \n",
      "    temp=''\n",
      "    for str_headline in news_selected_date['News'].values:\n",
      "        temp+=str_headline  \n",
      "\n",
      "    headlines.append(temp)\n",
      "headlines[0]\n",
      "headlines[-1]\n",
      "len(headlines)\n",
      "headlines_df = pd.DataFrame(data=headlines, index=dj.Date, columns='headlines')\n",
      "headlines_df = pd.DataFrame(data=headlines, index=dj.Date)\n",
      "headlines_df\n",
      "headlines_df = pd.DataFrame(data=headlines, index=dj.Date,columns=['headlines'])\n",
      "headlines_df\n",
      "headlines_df['headlines']\n",
      "headlines_df['headlines'].values\n",
      "list(headlines_df['headlines'].values)\n",
      "list(headlines_df['headlines'].values)[0]\n",
      "list(headlines_df['headlines'].values)[1]\n",
      "price\n",
      "len(price)\n",
      "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
      "contractions = { \n",
      "\"ain't\": \"am not\",\n",
      "\"aren't\": \"are not\",\n",
      "\"can't\": \"cannot\",\n",
      "\"can't've\": \"cannot have\",\n",
      "\"'cause\": \"because\",\n",
      "\"could've\": \"could have\",\n",
      "\"couldn't\": \"could not\",\n",
      "\"couldn't've\": \"could not have\",\n",
      "\"didn't\": \"did not\",\n",
      "\"doesn't\": \"does not\",\n",
      "\"don't\": \"do not\",\n",
      "\"hadn't\": \"had not\",\n",
      "\"hadn't've\": \"had not have\",\n",
      "\"hasn't\": \"has not\",\n",
      "\"haven't\": \"have not\",\n",
      "\"he'd\": \"he would\",\n",
      "\"he'd've\": \"he would have\",\n",
      "\"he'll\": \"he will\",\n",
      "\"he's\": \"he is\",\n",
      "\"how'd\": \"how did\",\n",
      "\"how'll\": \"how will\",\n",
      "\"how's\": \"how is\",\n",
      "\"i'd\": \"i would\",\n",
      "\"i'll\": \"i will\",\n",
      "\"i'm\": \"i am\",\n",
      "\"i've\": \"i have\",\n",
      "\"isn't\": \"is not\",\n",
      "\"it'd\": \"it would\",\n",
      "\"it'll\": \"it will\",\n",
      "\"it's\": \"it is\",\n",
      "\"let's\": \"let us\",\n",
      "\"ma'am\": \"madam\",\n",
      "\"mayn't\": \"may not\",\n",
      "\"might've\": \"might have\",\n",
      "\"mightn't\": \"might not\",\n",
      "\"must've\": \"must have\",\n",
      "\"mustn't\": \"must not\",\n",
      "\"needn't\": \"need not\",\n",
      "\"oughtn't\": \"ought not\",\n",
      "\"shan't\": \"shall not\",\n",
      "\"sha'n't\": \"shall not\",\n",
      "\"she'd\": \"she would\",\n",
      "\"she'll\": \"she will\",\n",
      "\"she's\": \"she is\",\n",
      "\"should've\": \"should have\",\n",
      "\"shouldn't\": \"should not\",\n",
      "\"that'd\": \"that would\",\n",
      "\"that's\": \"that is\",\n",
      "\"there'd\": \"there had\",\n",
      "\"there's\": \"there is\",\n",
      "\"they'd\": \"they would\",\n",
      "\"they'll\": \"they will\",\n",
      "\"they're\": \"they are\",\n",
      "\"they've\": \"they have\",\n",
      "\"wasn't\": \"was not\",\n",
      "\"we'd\": \"we would\",\n",
      "\"we'll\": \"we will\",\n",
      "\"we're\": \"we are\",\n",
      "\"we've\": \"we have\",\n",
      "\"weren't\": \"were not\",\n",
      "\"what'll\": \"what will\",\n",
      "\"what're\": \"what are\",\n",
      "\"what's\": \"what is\",\n",
      "\"what've\": \"what have\",\n",
      "\"where'd\": \"where did\",\n",
      "\"where's\": \"where is\",\n",
      "\"who'll\": \"who will\",\n",
      "\"who's\": \"who is\",\n",
      "\"won't\": \"will not\",\n",
      "\"wouldn't\": \"would not\",\n",
      "\"you'd\": \"you would\",\n",
      "\"you'll\": \"you will\",\n",
      "\"you're\": \"you are\"\n",
      "}\n",
      "def clean_text(text, remove_stopwords = True):\n",
      "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
      "    \n",
      "    # Convert words to lower case\n",
      "    text = text.lower()\n",
      "    \n",
      "    # Replace contractions with their longer forms \n",
      "    if True:\n",
      "        text = text.split()\n",
      "        new_text = []\n",
      "        for word in text:\n",
      "            if word in contractions:\n",
      "                new_text.append(contractions[word])\n",
      "            else:\n",
      "                new_text.append(word)\n",
      "        text = \" \".join(new_text)\n",
      "    \n",
      "    # Format words and remove unwanted characters\n",
      "    text = re.sub(r'&amp;', '', text) \n",
      "    text = re.sub(r'0,0', '00', text) \n",
      "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
      "    text = re.sub(r'\\'', ' ', text)\n",
      "    text = re.sub(r'\\$', ' $ ', text)\n",
      "    text = re.sub(r'u s ', ' united states ', text)\n",
      "    text = re.sub(r'u n ', ' united nations ', text)\n",
      "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
      "    text = re.sub(r'j k ', ' jk ', text)\n",
      "    text = re.sub(r' s ', ' ', text)\n",
      "    text = re.sub(r' yr ', ' year ', text)\n",
      "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
      "    text = re.sub(r'0km ', '0 km ', text)\n",
      "    \n",
      "    # Optionally, remove stop words\n",
      "    if remove_stopwords:\n",
      "        text = text.split()\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        text = [w for w in text if not w in stops]\n",
      "        text = \" \".join(text)\n",
      "\n",
      "    return text\n",
      "def clean_text(text, remove_stopwords = True):\n",
      "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
      "    \n",
      "    # Convert words to lower case\n",
      "    text = text.lower()\n",
      "    \n",
      "    # Replace contractions with their longer forms \n",
      "    if True:\n",
      "        text = text.split()\n",
      "        new_text = []\n",
      "        for word in text:\n",
      "            if word in contractions:\n",
      "                new_text.append(contractions[word])\n",
      "            else:\n",
      "                new_text.append(word)\n",
      "        text = \" \".join(new_text)\n",
      "    \n",
      "    # Format words and remove unwanted characters\n",
      "    text = re.sub(r'&amp;', '', text) \n",
      "    text = re.sub(r'0,0', '00', text) \n",
      "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
      "    text = re.sub(r'\\'', ' ', text)\n",
      "    text = re.sub(r'\\$', ' $ ', text)\n",
      "    text = re.sub(r'u s ', ' united states ', text)\n",
      "    text = re.sub(r'u n ', ' united nations ', text)\n",
      "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
      "    text = re.sub(r'j k ', ' jk ', text)\n",
      "    text = re.sub(r' s ', ' ', text)\n",
      "    text = re.sub(r' yr ', ' year ', text)\n",
      "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
      "    text = re.sub(r'0km ', '0 km ', text)\n",
      "    \n",
      "    # Optionally, remove stop words\n",
      "    if remove_stopwords:\n",
      "        text = text.split()\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        text = [w for w in text if not w in stops]\n",
      "        text = \" \".join(text)\n",
      "\n",
      "    return text\n",
      "headlines_df['headlines'].map(clean_text)\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tqdm\n",
      "import re\n",
      "headlines_df['headlines'].map(clean_text)\n",
      "import os\n",
      "import sys\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.preprocessing.sequence import pad_sequences\n",
      "from keras.utils import to_categorical\n",
      "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
      "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
      "from keras.models import Model\n",
      "from keras.initializers import Constant\n",
      "import tqdm\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "headlines_df['headlines'].map(clean_text)\n",
      "headlines_df['cleaned_headlines']=headlines_df['headlines'].map(clean_text)\n",
      "headlines_df\n",
      "def clean_text(text, remove_stopwords = True):\n",
      "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
      "    \n",
      "    # Convert words to lower case\n",
      "    text = text.lower()\n",
      "    \n",
      "    # Replace contractions with their longer forms \n",
      "    if True:\n",
      "        text = text.split()\n",
      "        new_text = []\n",
      "        for word in text:\n",
      "            if word in contractions and len(word)>1:\n",
      "                new_text.append(contractions[word])\n",
      "            else:\n",
      "                new_text.append(word)\n",
      "        text = \" \".join(new_text)\n",
      "    \n",
      "    # Format words and remove unwanted characters\n",
      "    text = re.sub(r'&amp;', '', text) \n",
      "    text = re.sub(r'0,0', '00', text) \n",
      "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
      "    text = re.sub(r'\\'', ' ', text)\n",
      "    text = re.sub(r'\\$', ' $ ', text)\n",
      "    text = re.sub(r'u s ', ' united states ', text)\n",
      "    text = re.sub(r'u n ', ' united nations ', text)\n",
      "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
      "    text = re.sub(r'j k ', ' jk ', text)\n",
      "    text = re.sub(r' s ', ' ', text)\n",
      "    text = re.sub(r' yr ', ' year ', text)\n",
      "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
      "    text = re.sub(r'0km ', '0 km ', text)\n",
      "    \n",
      "    # Optionally, remove stop words\n",
      "    if remove_stopwords:\n",
      "        text = text.split()\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        text = [w for w in text if not w in stops]\n",
      "        text = \" \".join(text)\n",
      "\n",
      "    return text\n",
      "headlines_df['cleaned_headlines']=headlines_df['headlines'].map(clean_text)\n",
      "headlines_df\n",
      "def clean_text(text, remove_stopwords = True):\n",
      "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
      "    \n",
      "    # Convert words to lower case\n",
      "    text = text.lower()\n",
      "    \n",
      "    # Replace contractions with their longer forms \n",
      "    if True:\n",
      "        text = text.split()\n",
      "        new_text = []\n",
      "        for word in text:\n",
      "            if word in contractions:\n",
      "                new_text.append(contractions[word])\n",
      "            else:\n",
      "                new_text.append(word)\n",
      "        text = \" \".join(new_text)\n",
      "    \n",
      "    # Format words and remove unwanted characters\n",
      "    text = re.sub(r'&amp;', '', text) \n",
      "    text = re.sub(r'0,0', '00', text) \n",
      "    text = re.sub(r'[_\"\\-;%()|.,+&=*%.,!?:#@\\[\\]]', ' ', text)\n",
      "    text = re.sub(r'\\'', ' ', text)\n",
      "    text = re.sub(r'\\$', ' $ ', text)\n",
      "    text = re.sub(r'u s ', ' united states ', text)\n",
      "    text = re.sub(r'u n ', ' united nations ', text)\n",
      "    text = re.sub(r'u k ', ' united kingdom ', text)\n",
      "    text = re.sub(r'j k ', ' jk ', text)\n",
      "    text = re.sub(r' s ', ' ', text)\n",
      "    text = re.sub(r' yr ', ' year ', text)\n",
      "    text = re.sub(r' l g b t ', ' lgbt ', text)\n",
      "    text = re.sub(r'0km ', '0 km ', text)\n",
      "    text = re.sub(r'b ', ' ', text)\n",
      "    # Optionally, remove stop words\n",
      "    if remove_stopwords:\n",
      "        text = text.split()\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        text = [w for w in text if not w in stops]\n",
      "        text = \" \".join(text)\n",
      "\n",
      "    return text\n",
      "headlines_df['cleaned_headlines']=headlines_df['headlines'].map(clean_text)\n",
      "headlines_df\n",
      "headlines_df['cleaned_headlines']\n",
      "headlines_df['cleaned_headlines'].values\n",
      "headlines_df['cleaned_headlines'].values[0]\n",
      "headlines_df['cleaned_headlines'].values[1]\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    a.update(headline)\n",
      "a\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    print(headline)\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    print(headline[0:20])\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    print(headline.split(\" \"))\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    print(headline.split(\" \")[0:3])\n",
      "a = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    a.update(headline.split(\" \")[0:3])\n",
      "a\n",
      "all_words = set()\n",
      "for headline in headlines_df['cleaned_headlines'].values:\n",
      "    all_words.update(headline.split(\" \"))\n",
      "len(all_words)\n",
      "BASE_DIR = ''\n",
      "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
      "MAX_SEQUENCE_LENGTH = 5\n",
      "MAX_NUM_WORDS = 55000\n",
      "EMBEDDING_DIM = 100\n",
      "VALIDATION_SPLIT = 0.2\n",
      "# first, build index mapping words in the embeddings set\n",
      "# to their embedding vector\n",
      "\n",
      "print('Indexing word vectors.')\n",
      "\n",
      "embeddings_index = {}\n",
      "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
      "    for line in f:\n",
      "        word, coefs = line.split(maxsplit=1)\n",
      "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
      "        embeddings_index[word] = coefs\n",
      "\n",
      "print('Found %s word vectors.' % len(embeddings_index))\n",
      "headlines_df['cleaned_headlines'].values\n",
      "texts = list(headlines_df['cleaned_headlines'].values)\n",
      "texts\n",
      "len(texts), len(price)\n",
      "# finally, vectorize the text samples into a 2D integer tensor\n",
      "#MAX_NUM_WORDS=180000\n",
      "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
      "tokenizer.fit_on_texts(texts)\n",
      "sequences = tokenizer.texts_to_sequences(texts)\n",
      "\n",
      "word_index = tokenizer.word_index\n",
      "print('Found %s unique tokens.' % len(word_index))\n",
      "sequences\n",
      "sequences[0]\n",
      "len(sequences[0])\n",
      "print(max(len(s) for s in sequences))\n",
      "print(min(len(s) for s in sequences))\n",
      "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
      "data\n",
      "data.shape\n",
      "data\n",
      "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "embedding_matrix.shape\n",
      "from keras.layers import Embedding\n",
      "\n",
      "embedding_layer = Embedding(len(word_index) + 1,\n",
      "                            EMBEDDING_DIM,\n",
      "                            weights=[embedding_matrix],\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(35)(x)  # global max pooling\n",
      "x = Flatten()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "BASE_DIR = ''\n",
      "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
      "MAX_SEQUENCE_LENGTH = 100\n",
      "MAX_NUM_WORDS = 55000\n",
      "EMBEDDING_DIM = 100\n",
      "VALIDATION_SPLIT = 0.2\n",
      "# first, build index mapping words in the embeddings set\n",
      "# to their embedding vector\n",
      "\n",
      "print('Indexing word vectors.')\n",
      "\n",
      "embeddings_index = {}\n",
      "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
      "    for line in f:\n",
      "        word, coefs = line.split(maxsplit=1)\n",
      "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
      "        embeddings_index[word] = coefs\n",
      "\n",
      "print('Found %s word vectors.' % len(embeddings_index))\n",
      "texts = list(headlines_df['cleaned_headlines'].values)\n",
      "len(texts), len(price)\n",
      "# finally, vectorize the text samples into a 2D integer tensor\n",
      "#MAX_NUM_WORDS=180000\n",
      "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
      "tokenizer.fit_on_texts(texts)\n",
      "sequences = tokenizer.texts_to_sequences(texts)\n",
      "\n",
      "word_index = tokenizer.word_index\n",
      "print('Found %s unique tokens.' % len(word_index))\n",
      "print(max(len(s) for s in sequences))\n",
      "print(min(len(s) for s in sequences))\n",
      "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
      "data\n",
      "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "embedding_matrix.shape\n",
      "from keras.layers import Embedding\n",
      "\n",
      "embedding_layer = Embedding(len(word_index) + 1,\n",
      "                            EMBEDDING_DIM,\n",
      "                            weights=[embedding_matrix],\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(35)(x)  # global max pooling\n",
      "x = Flatten()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "embedded_sequence\n",
      "embedded_sequences\n",
      "embedded_sequences.shape\n",
      "print('Preparing embedding matrix.')\n",
      "\n",
      "# prepare embedding matrix\n",
      "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
      "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i > MAX_NUM_WORDS:\n",
      "        continue\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "\n",
      "# load pre-trained word embeddings into an Embedding layer\n",
      "# note that we set trainable = False so as to keep the embeddings fixed\n",
      "embedding_layer = Embedding(num_words,\n",
      "                            EMBEDDING_DIM,\n",
      "                            embeddings_initializer=Constant(embedding_matrix),\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "\n",
      "print('Training model.')\n",
      "\n",
      "# train a 1D convnet with global maxpooling\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = GlobalMaxPooling1D()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "print('Preparing embedding matrix.')\n",
      "\n",
      "# prepare embedding matrix\n",
      "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
      "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i > MAX_NUM_WORDS:\n",
      "        continue\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "\n",
      "# load pre-trained word embeddings into an Embedding layer\n",
      "# note that we set trainable = False so as to keep the embeddings fixed\n",
      "embedding_layer = Embedding(num_words,\n",
      "                            EMBEDDING_DIM,\n",
      "                            embeddings_initializer=Constant(embedding_matrix),\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 10, input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(2, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "'''# train a 1D convnet with global maxpooling\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = GlobalMaxPooling1D()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "'''\n",
      "print('Preparing embedding matrix.')\n",
      "\n",
      "# prepare embedding matrix\n",
      "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
      "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i > MAX_NUM_WORDS:\n",
      "        continue\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "\n",
      "# load pre-trained word embeddings into an Embedding layer\n",
      "# note that we set trainable = False so as to keep the embeddings fixed\n",
      "embedding_layer = Embedding(num_words,\n",
      "                            EMBEDDING_DIM,\n",
      "                            embeddings_initializer=Constant(embedding_matrix),\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 10, input_length=10, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(2, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "'''# train a 1D convnet with global maxpooling\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = GlobalMaxPooling1D()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "'''\n",
      "num_words\n",
      "print('Preparing embedding matrix.')\n",
      "\n",
      "# prepare embedding matrix\n",
      "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
      "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i > MAX_NUM_WORDS:\n",
      "        continue\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "\n",
      "# load pre-trained word embeddings into an Embedding layer\n",
      "# note that we set trainable = False so as to keep the embeddings fixed\n",
      "embedding_layer = Embedding(num_words,\n",
      "                            EMBEDDING_DIM,\n",
      "                            embeddings_initializer=Constant(embedding_matrix),\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=10, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(2, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "'''# train a 1D convnet with global maxpooling\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = GlobalMaxPooling1D()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "'''\n",
      "embedded_sequences.shape\n",
      "BASE_DIR = ''\n",
      "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
      "MAX_SEQUENCE_LENGTH = 150\n",
      "MAX_NUM_WORDS = 55000\n",
      "EMBEDDING_DIM = 100\n",
      "VALIDATION_SPLIT = 0.2\n",
      "# first, build index mapping words in the embeddings set\n",
      "# to their embedding vector\n",
      "\n",
      "print('Indexing word vectors.')\n",
      "\n",
      "embeddings_index = {}\n",
      "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
      "    for line in f:\n",
      "        word, coefs = line.split(maxsplit=1)\n",
      "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
      "        embeddings_index[word] = coefs\n",
      "\n",
      "print('Found %s word vectors.' % len(embeddings_index))\n",
      "BASE_DIR = ''\n",
      "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
      "MAX_SEQUENCE_LENGTH = 150\n",
      "MAX_NUM_WORDS = 55000\n",
      "EMBEDDING_DIM = 100\n",
      "VALIDATION_SPLIT = 0.2\n",
      "# first, build index mapping words in the embeddings set\n",
      "# to their embedding vector\n",
      "\n",
      "print('Indexing word vectors.')\n",
      "\n",
      "embeddings_index = {}\n",
      "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
      "    for line in f:\n",
      "        word, coefs = line.split(maxsplit=1)\n",
      "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
      "        embeddings_index[word] = coefs\n",
      "\n",
      "print('Found %s word vectors.' % len(embeddings_index))\n",
      "texts = list(headlines_df['cleaned_headlines'].values)\n",
      "len(texts), len(price)\n",
      "# finally, vectorize the text samples into a 2D integer tensor\n",
      "#MAX_NUM_WORDS=180000\n",
      "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
      "tokenizer.fit_on_texts(texts)\n",
      "sequences = tokenizer.texts_to_sequences(texts)\n",
      "\n",
      "word_index = tokenizer.word_index\n",
      "print('Found %s unique tokens.' % len(word_index))\n",
      "print(max(len(s) for s in sequences))\n",
      "print(min(len(s) for s in sequences))\n",
      "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
      "data\n",
      "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "embedding_matrix.shape\n",
      "from keras.layers import Embedding\n",
      "\n",
      "embedding_layer = Embedding(len(word_index) + 1,\n",
      "                            EMBEDDING_DIM,\n",
      "                            weights=[embedding_matrix],\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "embedded_sequences.shape\n",
      "data.shape\n",
      "from keras.layers import Embedding\n",
      "\n",
      "embedding_layer = Embedding(len(word_index) + 1,\n",
      "                            EMBEDDING_DIM,\n",
      "                            weights=[embedding_matrix],\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "embedded_sequences.shape\n",
      "from keras.layers import Embedding\n",
      "print('Preparing embedding matrix.')\n",
      "\n",
      "# prepare embedding matrix\n",
      "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
      "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
      "for word, i in word_index.items():\n",
      "    if i > MAX_NUM_WORDS:\n",
      "        continue\n",
      "    embedding_vector = embeddings_index.get(word)\n",
      "    if embedding_vector is not None:\n",
      "        # words not found in embedding index will be all-zeros.\n",
      "        embedding_matrix[i] = embedding_vector\n",
      "\n",
      "# load pre-trained word embeddings into an Embedding layer\n",
      "# note that we set trainable = False so as to keep the embeddings fixed\n",
      "embedding_layer = Embedding(num_words,\n",
      "                            EMBEDDING_DIM,\n",
      "                            embeddings_initializer=Constant(embedding_matrix),\n",
      "                            input_length=MAX_SEQUENCE_LENGTH,\n",
      "                            trainable=False)\n",
      "embedding_matrix.shape\n",
      "embedding_matrix.shape, MAX_SEQUENCE_LENGTH\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(2, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "'''# train a 1D convnet with global maxpooling\n",
      "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
      "embedded_sequences = embedding_layer(sequence_input)\n",
      "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = MaxPooling1D(5)(x)\n",
      "x = Conv1D(128, 5, activation='relu')(x)\n",
      "x = GlobalMaxPooling1D()(x)\n",
      "x = Dense(128, activation='relu')(x)\n",
      "preds = Dense(len(labels_index), activation='softmax')(x)\n",
      "\n",
      "model = Model(sequence_input, preds)\n",
      "model.compile(loss='categorical_crossentropy',\n",
      "              optimizer='rmsprop',\n",
      "              metrics=['acc'])\n",
      "'''\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(2, activation='sigmoid'))\n",
      "\n",
      "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
      "model.fit(data,price)\n",
      "data.shape\n",
      "data\n",
      "price\n",
      "y = [[p] for p in price]\n",
      "y\n",
      "model.fit(data,y)\n",
      "model.fit(data,price)\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=4))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
      "data\n",
      "y = [[p] for p in price]\n",
      "model.fit(data,price)\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer='rs', metrics=['mse'])\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop, metrics=['mse'])\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop, metrics=['mse'])\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop, metrics=['mse'])\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
      "model.fit(data,price,batch)\n",
      "model.fit(data,price,batch_size=32)\n",
      "model.summary()\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, 100, input_length=150, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 50, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.fit(data,price,batch_size=32)\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 2, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(1000))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.fit(data,price,batch_size=32)\n",
      "model.summary()\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 2, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 2, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.summary()\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 2, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.summary()\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 2, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(LSTM(100))\n",
      "#model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.summary()\n",
      "model.fit(data, price, batch_size=2)\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 3, activation='relu'))\n",
      "#model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(LSTM(128))\n",
      "#model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.summary()\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 3, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(LSTM(128))\n",
      "#model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['mse'])\n",
      "model.summary()\n",
      "model.fit(data, price, batch_size=2)\n",
      "# Normalize opening prices (target values)\n",
      "max_price = max(price)\n",
      "min_price = min(price)\n",
      "mean_price = np.mean(price)\n",
      "def normalize(price):\n",
      "    return ((price-min_price)/(max_price-min_price))\n",
      "# Normalize opening prices (target values)\n",
      "max_price = max(price)\n",
      "min_price = min(price)\n",
      "mean_price = np.mean(price)\n",
      "def normalize(price):\n",
      "    return ((price-min_price)/(max_price-min_price))\n",
      "norm_price = []\n",
      "for p in price:\n",
      "    norm_price.append(normalize(p))\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 3, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(LSTM(128))\n",
      "#model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001,clipvalue=1.0), metrics=['mse'])\n",
      "model.fit(data, norm_price, batch_size=2)\n",
      "print('Training model.')\n",
      "\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
      "from keras import optimizers\n",
      "from keras.optimizers import Adam, SGD, RMSprop\n",
      "\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(num_words, EMBEDDING_DIM , input_length=MAX_SEQUENCE_LENGTH, weights= [embedding_matrix], trainable=False))\n",
      "\n",
      "#model.add(Dropout(0.2))\n",
      "model.add(Conv1D(64, 3, activation='relu'))\n",
      "model.add(MaxPooling1D(pool_size=2))\n",
      "model.add(Conv1D(64, 5, activation='relu'))\n",
      "model.add(LSTM(128))\n",
      "#model.add(Dense(600))\n",
      "model.add(Dense(1))\n",
      "\n",
      "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001,clipvalue=1.0), metrics=['mse'])\n",
      "model.summary()\n",
      "# Normalize opening prices (target values)\n",
      "max_price = max(price)\n",
      "min_price = min(price)\n",
      "mean_price = np.mean(price)\n",
      "def normalize(price):\n",
      "    return ((price-min_price)/(max_price-min_price))\n",
      "norm_price = []\n",
      "for p in price:\n",
      "    norm_price.append(normalize(p))\n",
      "model.fit(data, norm_price, batch_size=2)\n",
      "model.fit(data, norm_price, batch_size=32, epoch=10)\n",
      "model.fit(data, norm_price, batch_size=32, epochs=10)\n",
      "model.fit(data, norm_price, batch_size=500, epochs=10, validation_split=0.15)\n",
      "history\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
